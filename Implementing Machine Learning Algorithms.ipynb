{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Classification </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "data = pd.read_csv('/Users/jeremynixon/Dropbox/python/Oracle Development/iris.data', header=None)\n",
    "# iris = data[:100]\n",
    "iris=data\n",
    "y = np.array(iris[4])\n",
    "new_y = []\n",
    "for i in y:\n",
    "    if i == 'Iris-setosa':\n",
    "        new_y.append(1)\n",
    "    elif i == 'Iris-versicolor':\n",
    "        new_y.append(0)\n",
    "    elif i == 'Iris-virginica':\n",
    "        new_y.append(2)\n",
    "y = np.array(new_y)\n",
    "x = preprocessing.scale(np.array(iris.drop([4], 1)))\n",
    "# x = np.hstack((np.ones((x.shape[0],1)),x))\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "Y = df['quality'].values\n",
    "le = preprocessing.LabelEncoder().fit(Y) \n",
    "Y = le.transform(Y)\n",
    "df = preprocessing.scale(df.drop('quality',1))\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(df, Y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Logistic Regression </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, lr=.001, num_iters=10000):\n",
    "    # Add Bias\n",
    "    x_train = np.column_stack((np.ones(len(x_train)), x_train))\n",
    "    \n",
    "    # Collect Shapes\n",
    "    nrow, ncol = x_train.shape\n",
    "    nclasses = len(np.unique(y_train))\n",
    "    \n",
    "    # Initialize Weights\n",
    "    w = np.random.randn(ncol, nclasses)\n",
    "    \n",
    "    for iteration in xrange(num_iters):\n",
    "        # Forward\n",
    "        \n",
    "        # Weighted sum of features\n",
    "        output_raw = np.matmul(x_train, w)\n",
    "        \n",
    "        # Squash weighted sum values to be between 0 and 1\n",
    "        output_softmax = np.exp(output_raw)\n",
    "        output_softmax = output_softmax / np.sum(output_softmax, axis=1, keepdims=True)\n",
    "        \n",
    "        # Backward\n",
    "        output_softmax[range(len(x_train)), y_train] -= 1\n",
    "        gradient = np.matmul(x_train.T, output_softmax/nrow)\n",
    "        \n",
    "        # Update\n",
    "        w -= lr * gradient\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_lr(weights, x_test):\n",
    "    # Add Bias Column\n",
    "    x_test = np.column_stack((np.ones(len(x_test)), x_test))\n",
    "    \n",
    "    # Weighted sum of features\n",
    "    output_raw = np.matmul(x_test, weights)\n",
    "    \n",
    "    # Squash weighted sum values to be between 0 and 1\n",
    "    output_softmax = np.exp(output_raw)\n",
    "    output_softmax = output_softmax / np.sum(output_softmax, axis=1, keepdims=True)\n",
    "    \n",
    "    return np.argmax(output_softmax, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.503125\n"
     ]
    }
   ],
   "source": [
    "weights = logistic_regression(x_train, y_train)\n",
    "outputs = predict_lr(weights, x_test)\n",
    "accuracy = Counter(outputs - y_test)[0]/float(len(y_test))\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Neural Network </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_sgd(x_train, y_train, num_hidden=1000, lr=0.1, num_iters=1000, batch_size=32):\n",
    "    # Add bias\n",
    "    x_train = np.column_stack((np.ones(len(x_train)), x_train))\n",
    "    \n",
    "    # Get Important Shapes\n",
    "    n_row, n_col = np.shape(x_train)\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # Initialize Weight Matricies\n",
    "    w1 = np.random.randn(n_col, num_hidden) * .01\n",
    "    w2 = np.random.randn(num_hidden, n_classes) * .01\n",
    "    \n",
    "    # Iterate Through Backpropagation\n",
    "    for iteration in xrange(num_iters):\n",
    "        \n",
    "        stochastic_sample = np.random.randint(0, n_row-1, batch_size)\n",
    "            \n",
    "        x_batch = x_train[stochastic_sample]\n",
    "        y_batch = y_train[stochastic_sample]\n",
    "        \n",
    "        # Forward\n",
    "        hidden_raw = np.matmul(x_batch, w1)\n",
    "        hidden_relu = np.maximum(0, hidden_raw)\n",
    "        output_raw = np.matmul(hidden_relu, w2)\n",
    "        output_softmax = np.exp(output_raw)\n",
    "        output_softmax = output_softmax / np.sum(output_softmax, axis=1, keepdims=True)\n",
    "        \n",
    "        # Backward\n",
    "        output_softmax[range(batch_size), y_batch] -= 1\n",
    "        w2_gradient = np.matmul(hidden_relu.T, output_softmax/batch_size)\n",
    "        hidden_gradient = np.matmul(output_softmax/batch_size, w2.T)\n",
    "        hidden_gradient[hidden_relu <= 0] = 0\n",
    "        w1_gradient = np.matmul(x_batch.T, hidden_gradient)\n",
    "        \n",
    "        # Update Weights\n",
    "        w1 -= lr * w1_gradient\n",
    "        w2 -= lr * w2_gradient\n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_nn(w1, w2, x_test):\n",
    "    # Add Bias\n",
    "    x_test = np.column_stack((np.ones(len(x_test)), x_test))\n",
    "    # Forward\n",
    "    hidden_raw = np.matmul(x_test, w1)\n",
    "    hidden_relu = np.maximum(0, hidden_raw)\n",
    "    output_raw = np.matmul(hidden_relu, w2)\n",
    "    output_softmax = np.exp(output_raw)\n",
    "    output_softmax = output_softmax / np.sum(output_softmax, axis=1, keepdims=True)\n",
    "    return np.argmax(output_softmax, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59375\n"
     ]
    }
   ],
   "source": [
    "weights1, weights2 = neural_network_sgd(x_train, y_train)\n",
    "outputs = predict_nn(weights1, weights2, x_test)\n",
    "accuracy = Counter(outputs - y_test)[0]/float(len(y_test))\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Decision Tree </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tree(object):\n",
    "    def __init__(self, parents=None):\n",
    "        self.children = []\n",
    "        self.split_feature = None\n",
    "        self.split_feature_value = None\n",
    "        self.parents = parents\n",
    "        self.label = None\n",
    "\n",
    "def data_to_distribution(y_train):\n",
    "        types = set(y_train)\n",
    "        distribution = []\n",
    "        for i in types:\n",
    "            distribution.append(list(y_train).count(i)/float(len(y_train)))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "def entropy(distribution):\n",
    "    return -sum([p * math.log(p,2) for p in distribution])\n",
    "\n",
    "\n",
    "def split_data(x_train, y_train, feature_index):\n",
    "    attribute_values = x_train[:,feature_index]\n",
    "    for attribute in set(attribute_values):\n",
    "        data_subset = []\n",
    "        for index, point in enumerate(x_train):\n",
    "            if point[feature_index] == attribute:\n",
    "                data_subset.append([point, y_train[index]])\n",
    "        yield data_subset\n",
    "\n",
    "\n",
    "\n",
    "def gain(x_train, y_train, feature_index):\n",
    "    entropy_gain = entropy(data_to_distribution(y_train))\n",
    "    for data_subset in split_data(x_train, y_train, feature_index):\n",
    "        entropy_gain -= entropy(data_to_distribution([label\n",
    "                    for (point, label) in data_subset]))\n",
    "    return entropy_gain\n",
    "\n",
    "\n",
    "def homogeneous(y_train):\n",
    "    return len(set(y_train)) <= 1\n",
    "\n",
    "def majority_vote(y_train, node):\n",
    "    labels = y_train\n",
    "    choice = max(set(labels), key=list(labels).count)\n",
    "    node.label = choice\n",
    "    return node\n",
    "\n",
    "\n",
    "def build_decision_tree(x_train, y_train, root, remaining_features):\n",
    "    if homogeneous(y_train):\n",
    "        root.label = y_train[0]\n",
    "        return root\n",
    "    \n",
    "    if len(remaining_features) == 0:\n",
    "        return majority_vote(y_train, root)\n",
    "    \n",
    "    best_feature = max(remaining_features, key=lambda index: \n",
    "                       gain(x_train, y_train, index))\n",
    "    \n",
    "    if gain(x_train, y_train, best_feature) == 0:\n",
    "        return majority_vote(y_train, root)\n",
    "    \n",
    "    root.split_feature = best_feature\n",
    "    \n",
    "    for data_subset in split_data(x_train, y_train, best_feature):\n",
    "        child = Tree(parents = root)\n",
    "        child.split_feature_value = data_subset[0][0][best_feature]\n",
    "        root.children.append(child)\n",
    "        \n",
    "        new_x = np.array([point for (point, label) in data_subset])\n",
    "        new_y = np.array([label for (point, label) in data_subset])\n",
    "        \n",
    "        build_decision_tree(new_x, new_y, child, remaining_features - set([best_feature]))\n",
    "    \n",
    "    return root\n",
    "\n",
    "def decision_tree(x_train, y_train):\n",
    "    return build_decision_tree(x_train, y_train, Tree(), \n",
    "                               set(range(len(x_train[0]))))\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    nearest = (np.abs(array-value)).argmin()\n",
    "    return array[nearest]\n",
    "\n",
    "\n",
    "def classify(tree, point):\n",
    "    if tree.children == []:\n",
    "        return tree.label\n",
    "    else:\n",
    "        try:\n",
    "            matching_children = [child for child in tree.children\n",
    "                if child.split_feature_value == point[tree.split_feature]]\n",
    "            return classify(matching_children[0], point)\n",
    "        except:\n",
    "            array = [child.split_feature_value for child in tree.children]\n",
    "            point[tree.split_feature] = find_nearest(array, point[tree.split_feature])\n",
    "            matching_children = [child for child in tree.children\n",
    "                if child.split_feature_value == point[tree.split_feature]]\n",
    "            return classify(matching_children[0], point)\n",
    "\n",
    "\n",
    "def predict_dt(x_train, tree):\n",
    "    predicted_labels = [classify(tree, point) for point in x_train]\n",
    "    return predicted_labels\n",
    "\n",
    "def decision_tree_full(x_train, y_train, x_test):\n",
    "    tree = decision_tree(x_train, y_train)\n",
    "    predictions = text_classification(x_test, tree)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "tree = decision_tree(x_train, y_train)\n",
    "outputs = predict_dt(x_test, tree)\n",
    "accuracy = Counter(outputs - y_test)[0]/float(len(y_test))\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Random Forest </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    def __init__(self, parents=None):\n",
    "        self.children = []\n",
    "        self.split_feature = None\n",
    "        self.split_feature_value = None\n",
    "        self.parents = parents\n",
    "        self.label = None\n",
    "        \n",
    "def data_to_distribution(y_train):\n",
    "        types = set(y_train)\n",
    "        distribution = []\n",
    "        for i in types:\n",
    "            distribution.append(list(y_train).count(i)/float(len(y_train)))\n",
    "        return distribution\n",
    "    \n",
    "def entropy(distribution):\n",
    "    return -sum([p * math.log(p,2) for p in distribution])\n",
    "\n",
    "def split_data(x_train, y_train, feature_index):\n",
    "    attribute_values = x_train[:,feature_index]\n",
    "    for attribute in set(attribute_values):\n",
    "        data_subset = []\n",
    "        for index, point in enumerate(x_train):\n",
    "            if point[feature_index] == attribute:\n",
    "                data_subset.append([point, y_train[index]])\n",
    "        yield data_subset\n",
    "        \n",
    "def gain(x_train, y_train, feature_index):\n",
    "    entropy_gain = entropy(data_to_distribution(y_train))\n",
    "    for data_subset in split_data(x_train, y_train, feature_index):\n",
    "        entropy_gain -= entropy(data_to_distribution([label\n",
    "                    for (point, label) in data_subset]))\n",
    "    return entropy_gain\n",
    "\n",
    "def homogeneous(y_train):\n",
    "    return len(set(y_train)) <= 1\n",
    "\n",
    "def majority_vote(y_train, node):\n",
    "    labels = y_train\n",
    "    choice = max(set(labels), key=list(labels).count)\n",
    "    node.label = choice\n",
    "    return node\n",
    "\n",
    "def build_decision_tree(x_train, y_train, root, remaining_features):\n",
    "    remaining_features = np.array(list(remaining_features))\n",
    "    if homogeneous(y_train):\n",
    "        root.label = y_train[0]\n",
    "        return root\n",
    "    \n",
    "    if remaining_features.shape == 0:\n",
    "        return majority_vote(y_train, root)\n",
    "    \n",
    "    indices = np.random.choice(int(remaining_features.shape[0]), int(2*remaining_features.shape[0]/3), replace = False)\n",
    "\n",
    "    best_feature = max(remaining_features[indices], key=lambda index: \n",
    "                       gain(x_train, y_train, index))\n",
    "    remaining_features = set(remaining_features)\n",
    "    if gain(x_train, y_train, best_feature) == 0:\n",
    "        return majority_vote(y_train, root)\n",
    "    \n",
    "    root.split_feature = best_feature\n",
    "    \n",
    "    for data_subset in split_data(x_train, y_train, best_feature):\n",
    "        child = Tree(parents = root)\n",
    "        child.split_feature_value = data_subset[0][0][best_feature]\n",
    "        root.children.append(child)\n",
    "        \n",
    "        new_x = np.array([point for (point, label) in data_subset])\n",
    "        new_y = np.array([label for (point, label) in data_subset])\n",
    "        \n",
    "        build_decision_tree(new_x, new_y, child, remaining_features - set([best_feature]))\n",
    "    \n",
    "    return root\n",
    "\n",
    "def decision_tree(x_train, y_train):\n",
    "    return build_decision_tree(x_train, y_train, Tree(), \n",
    "                               set(range(len(x_train[0]))))\n",
    "def find_nearest(array, value):\n",
    "    nearest = (np.abs(array-value)).argmin()\n",
    "    return array[nearest]\n",
    "\n",
    "def classify(tree, point):\n",
    "    if tree.children == []:\n",
    "#         print \"label = %r\" %(tree.label)\n",
    "        return tree.label\n",
    "    else:\n",
    "        try:\n",
    "            matching_children = [child for child in tree.children\n",
    "                if child.split_feature_value == point[tree.split_feature]]\n",
    "            return classify(matching_children[0], point)\n",
    "        except:\n",
    "            array = [child.split_feature_value for child in tree.children]\n",
    "            point[tree.split_feature] = find_nearest(array, point[tree.split_feature])\n",
    "            matching_children = [child for child in tree.children\n",
    "                if child.split_feature_value == point[tree.split_feature]]\n",
    "            return classify(matching_children[0], point)\n",
    "        \n",
    "def text_classification(x_test, tree):\n",
    "    predicted_labels = [classify(tree, point) for point in x_test]\n",
    "    return predicted_labels\n",
    "\n",
    "def random_forest(x_train, y_train, x_test, n_estimators = 100):\n",
    "\n",
    "\n",
    "    x_train_copy = x_train\n",
    "    y_train_copy = y_train\n",
    "    x_test_copy = x_test\n",
    "    labels = []\n",
    "    sample = []\n",
    "    predictions = []\n",
    "    for i in range(n_estimators):\n",
    "        sample.append(np.random.choice(len(x_train), len(x_train),\n",
    "                                       replace=True))\n",
    "    for i in range(n_estimators):\n",
    "        x_train = x_train_copy.copy()\n",
    "        y_train = y_train_copy.copy()\n",
    "        x_test_copy = x_test_copy.copy()\n",
    "        \n",
    "        x = x_train[sample[i]]\n",
    "        y = y_train[sample[i]]\n",
    "        tree = decision_tree(x, y)\n",
    "        labels.append(text_classification(x_test_copy, tree))\n",
    "    \n",
    "    \n",
    "    for index in range(len(labels[0])):\n",
    "        prediction_dictionary = {}\n",
    "        for tree_result in range(len(labels)):\n",
    "            try:\n",
    "                prediction_dictionary[labels[tree_result][index]] += 1\n",
    "            except KeyError:\n",
    "                prediction_dictionary[labels[tree_result][index]] = 1\n",
    "        store = 0\n",
    "        for index, value in prediction_dictionary.iteritems():\n",
    "            if value > store:\n",
    "                store = value\n",
    "                chosen = index\n",
    "        predictions.append(chosen)\n",
    "        \n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.559375\n"
     ]
    }
   ],
   "source": [
    "outputs = random_forest(x_train, y_train, x_test)\n",
    "accuracy = Counter(outputs - y_test)[0]/float(len(y_test))\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> KNN </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn(x_train, y_train, x_test, k):\n",
    "    predictions = []\n",
    "    # Compute solution for each test datapoing\n",
    "    for datapoint in x_test:\n",
    "        distances = []\n",
    "        \n",
    "        # Get distance to every training set datapoint\n",
    "        for index, vector in enumerate(x_train):\n",
    "            distances.append([np.sum(np.sqrt((datapoint-vector)**2)), y_train[index]])\n",
    "                              \n",
    "        # Sort by distance\n",
    "        distances.sort()\n",
    "\n",
    "        # Plurality prediction\n",
    "        counts = {}\n",
    "        for i in xrange(k):\n",
    "            try:\n",
    "                counts[distances[i][1]] += 1\n",
    "            except:\n",
    "                counts[distances[i][1]] = 1\n",
    "        # Recover Maximum\n",
    "        base = 0\n",
    "        \n",
    "        for i in xrange(len(np.unique(y_train))):\n",
    "            try:\n",
    "                value = counts[i]\n",
    "                if value > base:\n",
    "                    prediction = i\n",
    "            except:\n",
    "                pass\n",
    "        predictions.append(prediction)\n",
    "    return predictions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.421875\n"
     ]
    }
   ],
   "source": [
    "outputs = knn(x_train, y_train, x_test, 5)\n",
    "accuracy = Counter(outputs - y_test)[0]/float(len(y_test))\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Regression </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jeremynixon/Dropbox/python/Oracle Development/iris.data', header=None)\n",
    "labels = np.array(df[0])\n",
    "features = np.array(df.drop([0, 4], 1))\n",
    "# features = np.column_stack((np.ones(features.shape[0]),features))\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(features, labels, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "Y = df['fixed acidity'].values\n",
    "df = preprocessing.scale(df.drop(['fixed acidity', 'quality'],1))\n",
    "x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(df, Y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Linear Regression </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_regression(x_train, y_train, lr=.00001, num_iters=1000):\n",
    "    # Bias\n",
    "    x_train = np.column_stack((np.ones(len(x_train)), x_train))\n",
    "    \n",
    "    # Initialize Weights\n",
    "    w = .01 * np.ones(x_train.shape[1])\n",
    "    \n",
    "    for iteration in xrange(num_iters):\n",
    "        raw_output = np.matmul(x_train, w)\n",
    "        diff = y_train - raw_output\n",
    "        grad = np.matmul(x_train.T, diff)\n",
    "        \n",
    "        w += lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_lin_reg(weights, x_test):\n",
    "    # Add Bias\n",
    "    x_test = np.column_stack((np.ones(len(x_test)), x_test))\n",
    "    \n",
    "    output = np.matmul(x_test, weights)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.442458032338\n"
     ]
    }
   ],
   "source": [
    "weights = linear_regression(x_train, y_train)\n",
    "outputs = predict_lin_reg(weights, x_test)\n",
    "avg_error = np.mean(np.abs(outputs-y_test))\n",
    "print avg_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> KNN Regressor </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn(x_train, y_train, x_test, k):\n",
    "    predictions = []\n",
    "    # Compute solution for each test datapoing\n",
    "    for datapoint in x_test:\n",
    "        distances = []\n",
    "        \n",
    "        # Get distance to every training set datapoint\n",
    "        for index, vector in enumerate(x_train):\n",
    "            distances.append([np.sum(np.sqrt((datapoint-vector)**2)), y_train[index]])\n",
    "                              \n",
    "        # Sort by distance\n",
    "        distances.sort()\n",
    "\n",
    "        # Aggregate neighboring values\n",
    "        average = 0\n",
    "        for i in xrange(k):\n",
    "            average += distances[i][1]\n",
    "        average = average / float(k)\n",
    "        predictions.append(average)\n",
    "    return predictions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.537375\n"
     ]
    }
   ],
   "source": [
    "outputs = knn(x_train, y_train, x_test, 5)\n",
    "avg_error = np.mean(np.abs(outputs-y_test))\n",
    "print avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
